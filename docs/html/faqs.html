<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Frequently Asked Questions &mdash; COMET 2.0.0 documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/comet.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/sphinx_highlight.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="COMET Metrics" href="models.html" />
    <link rel="prev" title="Running COMET" href="running.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html" class="icon icon-home">
            COMET
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="running.html">Running COMET</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Frequently Asked Questions</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#is-there-a-theoretical-range-of-values-for-the-comet-regressor">Is there a theoretical range of values for the COMET regressor?</a></li>
<li class="toctree-l2"><a class="reference internal" href="#which-comet-model-should-i-use">Which COMET model should I use?</a></li>
<li class="toctree-l2"><a class="reference internal" href="#where-can-i-find-the-data-used-to-train-comet-models">Where can I find the data used to train COMET models?</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#direct-assessments">Direct Assessments</a></li>
<li class="toctree-l3"><a class="reference internal" href="#direct-assessments-relative-ranks">Direct Assessments: Relative Ranks</a></li>
<li class="toctree-l3"><a class="reference internal" href="#direct-assessment-scalar-quality-metric">Direct Assessment + Scalar Quality Metric:</a></li>
<li class="toctree-l3"><a class="reference internal" href="#multidimensional-quality-metrics">Multidimensional Quality Metrics:</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="models.html">COMET Metrics</a></li>
<li class="toctree-l1"><a class="reference internal" href="models.html#available-evaluation-models">Available Evaluation Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="training.html">Train your own Metric</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">COMET</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Frequently Asked Questions</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/faqs.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="frequently-asked-questions">
<h1>Frequently Asked Questions<a class="headerlink" href="#frequently-asked-questions" title="Permalink to this heading">ïƒ</a></h1>
<p>Since we released COMET we have received several questions related to interpretabilty of the scores and usage. In this section we try to address these questions the best we can!</p>
<section id="is-there-a-theoretical-range-of-values-for-the-comet-regressor">
<h2>Is there a theoretical range of values for the COMET regressor?<a class="headerlink" href="#is-there-a-theoretical-range-of-values-for-the-comet-regressor" title="Permalink to this heading">ïƒ</a></h2>
<p>Before we dig deeper into details about COMET scores I would like to clarify something:</p>
<p><em>Absolute scores via automatic metrics are meaningless (what does 31 BLEU mean without context? it can be both awesome score for News EN-Finnish or really bad score for EN-French), and pretrained metrics only amplify it by using different scales for different languages and especially different domains.</em></p>
<p>Check <a class="reference external" href="https://aclanthology.org/2021.wmt-1.57/">[Kocmi et al. 2021]</a> and our discussion here: <a class="reference external" href="https://github.com/Unbabel/COMET/issues/18">#18</a></p>
<p>Most COMET models are trained to regress on a specific quality assessment and in most cases those scores are normalized for each annotator using a <a class="reference external" href="https://www.simplypsychology.org/z-score.html">z-score transformation</a>. The score itself has no direct interpretation but they correctly rank translations and systems according to their quality.</p>
<p>Also, depending on the data that they were used to train, different models might have different score ranges. We observed that most scores for our <code class="docutils literal notranslate"><span class="pre">Unbabel/wmt20-comet-da</span></code> fall between -1.5 and 1, while the new <code class="docutils literal notranslate"><span class="pre">Unbabel/wmt22-comet-da</span></code> model is normalized to score everything between 0 and 1. This means that you will observe larger score differences using <code class="docutils literal notranslate"><span class="pre">Unbabel/wmt20-comet-da</span></code>. Because of that <strong>it is important to use the <code class="docutils literal notranslate"><span class="pre">comet-compare</span></code> command to obtain statistical significance.</strong></p>
</section>
<section id="which-comet-model-should-i-use">
<h2>Which COMET model should I use?<a class="headerlink" href="#which-comet-model-should-i-use" title="Permalink to this heading">ïƒ</a></h2>
<p><strong>For general purpose MT evaluation</strong> we recommend you to use <code class="docutils literal notranslate"><span class="pre">Unbabel/wmt22-comet-da</span></code>. This is the most <em>stable</em> model we have. It is an improved version of our previous model <code class="docutils literal notranslate"><span class="pre">Unbabel/wmt20-comet-da</span></code>.</p>
<p><strong>For evaluating models without a reference</strong> we recommend the <code class="docutils literal notranslate"><span class="pre">Unbabel/wmt20-comet-qe-da</span></code> for higher correlations with DA, and to <a class="reference external" href="https://github.com/Unbabel/COMET/blob/master/MODELS.md">download <code class="docutils literal notranslate"><span class="pre">wmt21-comet-qe-mqm</span></code></a> for higher correlations with MQM.</p>
</section>
<section id="where-can-i-find-the-data-used-to-train-comet-models">
<h2>Where can I find the data used to train COMET models?<a class="headerlink" href="#where-can-i-find-the-data-used-to-train-comet-models" title="Permalink to this heading">ïƒ</a></h2>
<section id="direct-assessments">
<h3>Direct Assessments<a class="headerlink" href="#direct-assessments" title="Permalink to this heading">ïƒ</a></h3>
<table border="1" class="docutils">
<thead>
<tr>
<th style="text-align: center;">year</th>
<th style="text-align: center;">data</th>
<th style="text-align: center;">paper</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">2017</td>
<td style="text-align: center;"><a href="https://unbabel-experimental-data-sets.s3.eu-west-1.amazonaws.com/comet/data/2017-da.tar.gz">ğŸ”—</a></td>
<td style="text-align: center;"><a href="https://aclanthology.org/W17-4717.pdf">Findings of the 2017 Conference on Machine Translation (WMT17)</a></td>
</tr>
<tr>
<td style="text-align: center;">2018</td>
<td style="text-align: center;"><a href="https://unbabel-experimental-data-sets.s3.eu-west-1.amazonaws.com/comet/data/2018-da.tar.gz">ğŸ”—</a></td>
<td style="text-align: center;"><a href="https://aclanthology.org/W18-6401.pdf">Findings of the 2018 Conference on Machine Translation (WMT18)</a></td>
</tr>
<tr>
<td style="text-align: center;">2019</td>
<td style="text-align: center;"><a href="https://unbabel-experimental-data-sets.s3.eu-west-1.amazonaws.com/comet/data/2019-da.tar.gz">ğŸ”—</a></td>
<td style="text-align: center;"><a href="https://aclanthology.org/W19-5301.pdf">Findings of the 2019 Conference on Machine Translation (WMT19)</a></td>
</tr>
<tr>
<td style="text-align: center;">2020</td>
<td style="text-align: center;"><a href="https://unbabel-experimental-data-sets.s3.eu-west-1.amazonaws.com/comet/data/2020-da.tar.gz">ğŸ”—</a></td>
<td style="text-align: center;"><a href="https://aclanthology.org/2020.wmt-1.1.pdf">Findings of the 2020 Conference on Machine Translation (WMT20)</a></td>
</tr>
<tr>
<td style="text-align: center;">2021</td>
<td style="text-align: center;"><a href="https://unbabel-experimental-data-sets.s3.eu-west-1.amazonaws.com/comet/data/2021-da.tar.gz">ğŸ”—</a></td>
<td style="text-align: center;"><a href="https://aclanthology.org/2021.wmt-1.1.pdf">Findings of the 2021 Conference on Machine Translation (WMT21)</a></td>
</tr>
<tr>
<td style="text-align: center;">2022</td>
<td style="text-align: center;"><a href="https://unbabel-experimental-data-sets.s3.eu-west-1.amazonaws.com/comet/data/2022-da.tar.gz">ğŸ”—</a></td>
<td style="text-align: center;"><a href="https://aclanthology.org/2022.wmt-1.1.pdf">Findings of the 2022 Conference on Machine Translation (WMT22)</a></td>
</tr>
</tbody>
</table><p>Another large source of DA annotations is the <a class="reference external" href="https://aclanthology.org/2022.lrec-1.530.pdf">MLQE-PE corpus</a> that is typically used for quality estimation shared tasks <a class="reference external" href="https://aclanthology.org/2020.wmt-1.79.pdf">(Specia et al. 2020</a><a class="reference external" href="https://aclanthology.org/2021.wmt-1.71.pdf">, 2021</a><a class="reference external" href="https://aclanthology.org/2022.wmt-1.3.pdf">; Zerva et al. 2022)</a>.</p>
<p>You can download MLQE-PE by using the following <a class="reference external" href="https://unbabel-experimental-data-sets.s3.eu-west-1.amazonaws.com/comet/data/mlqe-pe.tar.gz">ğŸ”—</a>.</p>
</section>
<section id="direct-assessments-relative-ranks">
<h3>Direct Assessments: Relative Ranks<a class="headerlink" href="#direct-assessments-relative-ranks" title="Permalink to this heading">ïƒ</a></h3>
<p>Before 2021 the WMT Metrics shared task used relative ranks to evaluate metrics.</p>
<p>Relative ranks can be created when we have at least two DA scores for translations of the same source input, by converting those DA scores into a relative ranking judgement, if the difference in DA scores allows conclusion that one translation is better than the other (usually atleast 25 points).</p>
<p>To make it easier to replicate results from previous Metrics shared tasks (2017-2020) you can find the preprocessed DA relative ranks in the table below:</p>
<table border="1" class="docutils">
<thead>
<tr>
<th style="text-align: center;">year</th>
<th style="text-align: center;">relative ranks</th>
<th style="text-align: center;">paper</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">2017</td>
<td style="text-align: center;"><a href="https://unbabel-experimental-data-sets.s3.eu-west-1.amazonaws.com/wmt/2017-daRR.csv.tar.gz">ğŸ”—</a></td>
<td style="text-align: center;"><a href="https://statmt.org/wmt17/pdf/WMT55.pdf">Results of the WMT17 Metrics Shared Task</a></td>
</tr>
<tr>
<td style="text-align: center;">2018</td>
<td style="text-align: center;"><a href="https://unbabel-experimental-data-sets.s3.eu-west-1.amazonaws.com/wmt/2018-daRR.csv.tar.gz">ğŸ”—</a></td>
<td style="text-align: center;"><a href="https://statmt.org/wmt18/pdf/WMT078.pdf">Results of the WMT18 Metrics Shared Task</a></td>
</tr>
<tr>
<td style="text-align: center;">2019</td>
<td style="text-align: center;"><a href="https://unbabel-experimental-data-sets.s3.eu-west-1.amazonaws.com/wmt/2019-daRR.csv.tar.gz">ğŸ”—</a></td>
<td style="text-align: center;"><a href="https://statmt.org/wmt19/pdf/53/WMT02.pdf">Results of the WMT19 Metrics Shared Task</a></td>
</tr>
<tr>
<td style="text-align: center;">2020</td>
<td style="text-align: center;"><a href="https://unbabel-experimental-data-sets.s3.eu-west-1.amazonaws.com/wmt/2020-daRR.csv.tar.gz">ğŸ”—</a></td>
<td style="text-align: center;"><a href="https://aclanthology.org/2020.wmt-1.77.pdf">Results of the WMT20 Metrics Shared Task</a></td>
</tr>
</tbody>
</table></section>
<section id="direct-assessment-scalar-quality-metric">
<h3>Direct Assessment + Scalar Quality Metric:<a class="headerlink" href="#direct-assessment-scalar-quality-metric" title="Permalink to this heading">ïƒ</a></h3>
<p>In 2022, several changes were made to the annotation procedure used in the WMT Translation task. In contrast to the standard DA (sliding scale from 0-100) used in previous years, in 2022 annotators performed DA+SQM (Direct Assessment + Scalar Quality Metric). In DA+SQM, the annotators still provide a raw score between 0 and 100, but also are presented with seven labeled tick marks. DA+SQM helps to stabilize scores across annotators (as compared to DA).</p>
<table border="1" class="docutils">
<thead>
<tr>
<th style="text-align: center;">year</th>
<th style="text-align: center;">data</th>
<th style="text-align: center;">paper</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">2022</td>
<td style="text-align: center;"><a href="https://unbabel-experimental-data-sets.s3.eu-west-1.amazonaws.com/comet/data/2022-sqm.tar.gz">ğŸ”—</a></td>
<td style="text-align: center;"><a href="https://aclanthology.org/2022.wmt-1.1.pdf">Findings of the 2022 Conference on Machine Translation (WMT22)</a></td>
</tr>
</tbody>
</table></section>
<section id="multidimensional-quality-metrics">
<h3>Multidimensional Quality Metrics:<a class="headerlink" href="#multidimensional-quality-metrics" title="Permalink to this heading">ïƒ</a></h3>
<p>Since 2021 the WMT Metrics task decided to perform they own expert-based evaluation based on <em>Multidimensional Quality Metrics (MQM)</em> framework. In the table below you can find MQM annotations from previous years.</p>
<table border="1" class="docutils">
<thead>
<tr>
<th style="text-align: center;">year</th>
<th style="text-align: center;">data</th>
<th style="text-align: center;">paper</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">2020</td>
<td style="text-align: center;"><a href="https://unbabel-experimental-data-sets.s3.eu-west-1.amazonaws.com/comet/data/2020-mqm.tar.gz">ğŸ”—</a></td>
<td style="text-align: center;"><a href="https://aclanthology.org/2021.tacl-1.87.pdf">A Large-Scale Study of Human Evaluation for Machine Translation</a></td>
</tr>
<tr>
<td style="text-align: center;">2021</td>
<td style="text-align: center;"><a href="https://unbabel-experimental-data-sets.s3.eu-west-1.amazonaws.com/comet/data/2021-mqm.tar.gz">ğŸ”—</a></td>
<td style="text-align: center;"><a href="https://aclanthology.org/2021.wmt-1.73.pdf">Results of the WMT21 Metrics Shared Task</a></td>
</tr>
<tr>
<td style="text-align: center;">2022</td>
<td style="text-align: center;"><a href="https://unbabel-experimental-data-sets.s3.eu-west-1.amazonaws.com/comet/data/2022-mqm.tar.gz">ğŸ”—</a></td>
<td style="text-align: center;"><a href="https://aclanthology.org/2022.wmt-1.2.pdf">Results of the WMT22 Metrics Shared Task</a></td>
</tr>
</tbody>
</table><p><strong>Note:</strong> You can find the original MQM data <a class="reference external" href="https://github.com/google/wmt-mqm-human-evaluation">here</a>.</p>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="running.html" class="btn btn-neutral float-left" title="Running COMET" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="models.html" class="btn btn-neutral float-right" title="COMET Metrics" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, Unbabel. All rights reserved.Source code available under Apache License 2.0.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>